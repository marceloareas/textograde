Index: fine-tunning/lora/lora-peft-v2.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># https://huggingface.co/spaces/PEFT/sequence-classification/blob/main/LoRA.ipynb\nimport argparse\nimport os\n\nimport torch\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\nfrom peft import (\n    get_peft_config,\n    get_peft_model,\n    get_peft_model_state_dict,\n    set_peft_model_state_dict,\n    LoraConfig,\n    PeftType,\n    PrefixTuningConfig,\n    PromptEncoderConfig,\n)\n\nimport evaluate\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    get_linear_schedule_with_warmup,\n    set_seed,\n)\nfrom tqdm import tqdm\n\n## todo: - consigo dividir os dados em treino e teste?\n## todo: - consigo juntar todas as redaçẽos para fazer um classificador de nota geral?\nbatch_size = 4\nmodel_name_or_path = \"roberta-large\"\ntask = \"mrpc\"\npeft_type = PeftType.LORA\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnum_epochs = 5\nlr = 3e-4\npadding_side = \"right\"  ## todo: padding ser right ou left faz alguma diferença?\n\npeft_config = LoraConfig(\n    task_type=\"SEQ_CLS\",\n    inference_mode=False,\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.1,\n    use_dora=True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding=padding_side)\nif getattr(tokenizer, \"pad_token_id\") is None:\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\n# datasets = load_dataset(\"glue\", task)\ndatasets = load_dataset(\"parquet\", data_files=\"preprocessing/train_output-parquet.parquet\")\ndatasets_test = load_dataset(\"parquet\", data_files=\"preprocessing/test-parquet.parquet\")\nmetric = evaluate.load(\"accuracy\")\n\n\ndef tokenize(examples):\n    outputs = tokenizer(examples[\"texto\"], truncation=True, max_length=512)\n    return outputs\n\n\ntokenize_datasets = datasets.map(\n    tokenize,\n    batched=True,\n    remove_columns=[\"texto\", \"nota\"],\n)\n\n\ntokenize_datasets_test = datasets_test.map(\n    tokenize,\n    batched=True,\n    remove_columns=[\"texto\", \"nota\"],\n)\n\n# tokenize_datasets = tokenize_datasets.rename_column(\"label\", \"labels\")\n\n\ndef collate_fn(examples):\n    return tokenizer.pad(examples, padding=\"longest\", return_tensors=\"pt\")\n\n\ntrain_dataloader = DataLoader(\n    tokenize_datasets[\"train\"],\n    shuffle=True,\n    collate_fn=collate_fn,\n    batch_size=batch_size,\n)\neval_dataloader = DataLoader(\n    tokenize_datasets_test[\"test\"],\n    shuffle=False,\n    collate_fn=collate_fn,\n    batch_size=batch_size,\n)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name_or_path, return_dict=True, num_labels=10\n)\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\nprint(model)\n\noptimizer = AdamW(model.parameters(), lr=lr)\n\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=0.06 * (len(train_dataloader) * num_epochs),\n    num_training_steps=(len(train_dataloader) * num_epochs),\n)\n\nmodel.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    for step, batch in enumerate(train_dataloader):\n        batch.to(device)\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n    model.eval()\n    for step, batch in enumerate(tqdm(eval_dataloader)):\n        batch.to(device)\n        with torch.no_grad():\n            outputs = model(**batch)\n        predictions = outputs.logits.argmax(dim=-1)\n        predictions, references = predictions, batch[\"labels\"]\n        metric.add_batch(\n            predictions=predictions,\n            references=references,\n        )\n\n    eval_metric = metric.compute()\n    print(f\"epoch {epoch}:\", eval_metric)\n\nprint(\"finish!!\")\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/fine-tunning/lora/lora-peft-v2.py b/fine-tunning/lora/lora-peft-v2.py
--- a/fine-tunning/lora/lora-peft-v2.py	(revision 6836cff8f26f89457f9acd524df06ef27792a9f4)
+++ b/fine-tunning/lora/lora-peft-v2.py	(date 1719350399546)
@@ -1,4 +1,4 @@
-# https://huggingface.co/spaces/PEFT/sequence-classification/blob/main/LoRA.ipynb
+#https://huggingface.co/spaces/PEFT/sequence-classification/blob/main/LoRA.ipynb
 import argparse
 import os
 
Index: Makefile
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>COMPOSE_FILE := docker-compose.yml\nDOCKER_COMPOSE := docker-compose\n\nup:\n\t$(DOCKER_COMPOSE) -f $(COMPOSE_FILE) up --build\n\ndown:\n\t$(DOCKER_COMPOSE) -f $(COMPOSE_FILE) down --remove-orphans\n\nremove-logs:\n\tcd backend/src && sudo rm -r *_log.txt\n\nexecute-pipeline:\n\tcd textgrader-pt-br && python create_dataset.py\n\nsetup:\n\tpip install -r textgrader-pt-br/requirements.txt\n\nrun-all:\n\tcd textgrader-pt-br/scripts && \\\n\tpython create_dataset.py && \\\n\tpython extract_features.py && \\\n\tpython vectorize.py && \\\n\tpython fit_predict.py\n\nhugging-face:\n\thuggingface-cli login
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Makefile b/Makefile
--- a/Makefile	(revision 6836cff8f26f89457f9acd524df06ef27792a9f4)
+++ b/Makefile	(date 1719348717889)
@@ -24,4 +24,7 @@
 	python fit_predict.py
 
 hugging-face:
-	huggingface-cli login
\ No newline at end of file
+	huggingface-cli login
+
+beautify-files:
+	black fine-tunning
\ No newline at end of file
